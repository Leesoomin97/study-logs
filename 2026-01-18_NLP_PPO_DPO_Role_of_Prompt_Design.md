# 1. 머리글

RLHF는 구조적으로 비용이 크고 복잡한 파이프라인을 요구한다. 특히 강화 학습 단계(PPO)는 구현과 운영 측면에서 부담이 크다. 이러한 문제의식에 따라 두 가지 방향의 발전이 등장했다.

- RLHF 자체를 단순화·대체하려는 시도 (DPO 등)  
- 학습이 아닌 사용 단계에서 모델을 조정하는 방법 (프롬프트 설계)

이번 글에서는 RLHF 이후 등장한 PPO와 DPO의 구조적 차이, 그리고 왜 프롬프트 설계가 별도의 축으로 자리 잡았는지를 정리해 보고자 한다.

---

# 2. PPO 단계

RLHF 파이프라인에서 가장 부담이 큰 단계는 PPO(Proximal Policy Optimization)를 사용하는 강화 학습 단계다.

이 단계의 목적은 명확하다. Reward Model이 높은 점수를 주는 방향으로 언어 모델의 정책(policy)를 점진적으로 업데이트하는 것이다.

언어 모델을 정책 \( \pi_\theta(y \mid x) \)로 보면, PPO는 다음 목적 함수를 최대화한다.

(여기에 수식/이미지 자리)

여기서 핵심은 **정책이 한 번에 크게 변하지 않도록 제한한다는 점**이다. 이는 언어 모델의 발산을 막기 위함이지만, 동시에 학습을 복잡하게 만들기도 한다.

---

# 3. PPO 기반 RLHF의 한계

PPO는 이론적으로 안정적이지만, LLM 환경에서는 다음 문제가 발생한다.

- Reward Model 학습 비용이 큼  
- PPO 학습이 느리고 불안정함  
- 하이퍼 파라미터 튜닝이 어려움  
- 학습 로그 및 디버깅 난이도 높음  

즉, RLHF는 성능은 좋지만 **스케일하기 어려운 구조**이다.  
이 한계를 해결하려는 시도가 바로 **DPO(Direct Preference Optimization)**이다.

---

# 4. DPO 방법

DPO는 *정책 경사 기반 강화 학습이 정말로 필요한가*에서 시작된 방법이다.  
이는 PPO 기반 RLHF와 비교할 때 다음과 같은 차이를 가진다.

- 강화 학습 루프 제거  
- Reward Model 불필요  
- 하나의 supervised objective로 학습 가능  

DPO의 목표는 RLHF의 목표를 유지하면서 **구현 복잡도만 제거하는 것**이다.  
핵심 아이디어는 **강화 학습 없이 선호도 데이터 자체를 직접 최적화 목표로 삼는 것**이다.

이로 인해 DPO는 학습 안정성, 재현성, 비용 측면에서 빠르게 확산되고 있다.

DPO가 이용하는 인간 선호도 데이터는 보통 다음 형태를 가진다.

- (x, y+, y-)
- y+ : 더 선호되는 응답  
- y- : 덜 선호되는 응답  

DPO는 다음 목적을 최대화한다.

(여기에 수식/이미지 자리)

이 식은 모델이 **선호되는 응답에 더 높은 확률을 주도록 직접 유도**한다.

---

# 5. PPO vs. DPO 선택 기준

| 항목 | PPO (RLHF) | DPO |
|------|------------|-----|
| 학습 방식 | 강화 학습 | 지도학습 |
| Reward Model | 필요 | 불필요 |
| 구현 난이도 | 높음 | 낮음 |
| 안정성 | 민감 | 비교적 안정 |
| 선호도 반영 | 간접적 | 직접적 |

PPO는 여전히 **복잡한 선호 구조나 다단계 보상 설계**가 필요한 경우 유효하지만,  
일반적인 LLM alignment에서는 **DPO가 더 실용적인 선택**이 되고 있다.

---

# 6. 프롬프트 설계

한편, 모든 조정을 학습으로 해결할 필요는 없다는 관점이 강화되며 **프롬프트 설계**가 등장했다.

프롬프트 설계는 모델 파라미터를 바꾸지 않고, **입력 문맥을 통해 모델의 행동을 제어**한다.  
이 접근은 LLM이 이미 충분히 많은 패턴을 학습했다는 가정을 전제로 한다.

프롬프트 설계가 효과적인 이유는 LLM이 입력 조건부 분포를 매우 민감하게 반영하기 때문이다.

-P(y | x, prompt)

Instruction, 예시, 제약 조건은 모두 확률 분포에 직접 영향을 미친다.  
이는 **ICL(In-Context Learning)**과 동일한 메커니즘 위에 있으며, 학습 비용 없이도 모델 행동을 조정할 수 있다.

---

# 7. 학습과 프롬프트의 역할 분담

| 구분 | 학습 기반 (DPO/RLHF) | 프롬프트 설계 |
|------|----------------------|---------------|
| 파라미터 변경 | O | X |
| 비용 | 높음 | 낮음 |
| 일관성 | 높음 | 프롬프트 의존 |
| 즉각성 | 낮음 | 높음 |
| 운영 난이도 | 높음 | 낮음 |

현실적인 시스템에서는  
**학습으로 기본 성향을 잡고, 프롬프트로 세부 동작을 조정하는 혼합 전략**이 일반적이다.

---

# 8. 마무리

RLHF 이후의 발전은 더 복잡한 강화 학습이 아니라, **같은 목표를 더 단순하게 달성하는 방향**으로 이루어졌다.

- PPO는 이론적으로 강력하지만 무거운 선택  
- DPO는 선호도 최적화를 단순화한 실용적인 대안  
- 프롬프트 설계는 학습을 대체·보완하는 사용 단계 전략  

이 세 축은 경쟁 관계가 아니라, **LLM을 실제 시스템에 통합하기 위한 서로 다른 레이어**이다.

---

