# 1. 머리글

대규모 언어 모델(LLM)은 단순히 크기가 커졌다는 이유만으로 잘 사용되고 있다고 평가받지 않는다. 사전학습만으로는 모델이 지시를 정확히 이해하지 못하며, 사용자가 원하는 방식대로 답변할 수도 없다.

이 문제를 해결하기 위해 등장한 것이 Instruction Tuning과 RLHF(Reinforcement Learning from Human Feedback)이다.

두 기법은 전혀 다른 목적을 갖지만, 결과적으로는 LLM의 조작 가능성을 결정짓는 핵심 요소가 된다. 이번 글에서는 이 두 기법이 어떤 데이터를 사용하고 어떤 신호를 학습하며, 실제로 모델을 어떻게 변화시키는지 구조적으로 분석해 보고자 한다.

---

# 2. LLM 문제 정의

사전학습 언어 모델은 아래 식에 따른 예측값을 잘 근사하는 모델에 불과하다.

(여기에 이미지 또는 수식 자리)

즉, 모델은 다음 단어를 잘 예상할 뿐, '요약해 줘', '설명해 줘', '단계별로 생각해 줘'와 같은 명령어(Task Instruction)를 잘 이해하도록 학습된 적이 없다.

이 한계를 해결하기 위해 본 글의 주제인 Instruction Tuning과 RLHF가 차례대로 도입되었다.

---

# 3. Instruction Tuning: 모델이 지시를 이해하게 만드는 과정

## (1) 개념

Instruction Tuning은 **명령어 + 입력 + 출력** 형식의 데이터로 모델을 지도학습하는 단계이다.  
예:

- “Summarize the following text” → 요약  
- “Translate to Korean” → 번역  
- “Extract the key entities” → 정보 추출  

즉, 모델이 **‘이 문장은 하나의 task 설명이다’**라는 개념을 학습하게 된다.

## (2) 데이터 구조 및 효과

Instruction Tuning 데이터셋은 다음과 같은 `(instruction, input, output)` 구조를 가진다.

포함되는 범주의 태스크는 다음과 같다.

- 분류  
- 요약  
- 정보 추출  
- 질의응답  
- 변환(번역, rewriting)  
- reasoning task  

데이터가 충분히 다양할수록 모델은 보지 못한 태스크에도 일반화를 할 수 있다.

Instruction Tuning을 적용하면 모델은 지시어를 구분하고, 태스크 경계를 파악하며, 문맥상 무엇을 해야 하는지 인식한다.  
이는 **ICL(In-Context Learning)** 능력을 강화하는 기반이 된다.

---

# 4. RLHF: 모델을 사용자가 원하는 방식으로 조정하는 과정

## (1) 전체 구조

Instruction Tuning이 ‘태스크 이해’에 해당한다면, RLHF는 ‘응답의 성향 조정’ 단계이다.  
일반적으로 아래 3단계를 따른다.

1. **Supervised Fine-Tuning (SFT)**: 고품질 답변을 정답처럼 학습  
2. **Reward Model(RM)** 학습: 인간이 더 선호하는 답변을 점수화  
3. **RL 최적화(PPO 등)**: 높은 보상(선호도)을 주는 답변을 생성하도록 학습  

이를 거치며 모델은 **사실 기반성, 안정성, 유용성** 등을 조정 받는다.

## (2) Reward Model 학습 구조 및 RL의 효과

Reward Model은 두 개의 답변 중 인간이 더 선호한 답변을 학습한다.

- RM(y_preferred) > RM(y_rejected)

이 신호를 통해 RM은 ‘어떤 답변이 더 바람직한지’를 학습한다.

SFT만으로는 잘못된 답변을 ‘피하라’는 신호를 줄 수 없기 때문에 마지막 단계에서 RL을 사용한다.

RL은:

- 선호도 기반 보상 학습 가능  
- 답변 스타일, 안전성 등 정답이 없는 영역 제어 가능  

즉, **RLHF는 실제 서비스에서 모델을 안전하게 사용하기 위한 필수 단계**이다.

---

# 5. Instruction Tuning과 RLHF의 역할 분리

| 항목 | Instruction Tuning | RLHF |
|------|---------------------|------|
| 목적 | 태스크 이해 | 응답 선호도 조정 |
| 학습 신호 | 지도학습(정답) | 선호도 기반 보상 |
| 효과 | 모델이 무엇을 해야 하는지 앎 | 어떻게 답해야 하는지 조정 |
| 데이터 | instruction-format | human preference data |

두 기법은 경쟁 관계가 아니라 **보완적 구조**이다.

- Instruction Tuning만 수행하면  
  → 지시는 이해하지만 **장황하거나 위험하거나 부정확한 답변**을 줄 수 있다.  
- RLHF만 수행하면  
  → 모델이 **태스크 자체를 이해하지 못해**, 보상 신호가 안정적으로 작동하지 않는다.

따라서 LLM은 **Instruction Tuning으로 지시 이해**, **RLHF로 응답 품질 조정**이라는 2단 구조를 갖는다.

---

# 6. 마무리

Instruction Tuning과 RLHF는 현대 LLM이 사용 가능한 모델이 되는 데 핵심적인 역할을 한다.  

- Instruction Tuning → 모델이 **무엇을 해야 하는지** 이해  
- RLHF → 모델이 **어떻게 답해야 하는지** 학습  

이 두 과정은 LLM의 기능적 측면(태스크 이해)과 정성적 측면(사용자 선호도)을 각각 보완하는 축이다.  
그렇기에 이 둘을 이용하는 구조는 ChatGPT, Claude, Gemini 등 현대 대규모 모델의 **표준 기법**이 되었다.

---
