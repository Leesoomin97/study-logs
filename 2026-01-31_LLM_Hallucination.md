# 1. 머리글

RAG, Tool Use, Agent 구조를 통해 LLM은 더 많은 정보를 활용하고 복잡한 작업을 수행할 수 있게 되었다. 그러나 시스템이 고도화될수록 LLM의 출력에 대한 신뢰성(Reliability)가 대두되었다.

LLM은 문법적으로 자연스럽고 설득력 있는 문장을 생성하는데 뛰어난 성능을 보였으나, 이는 어디까지나 형식에 관해서이지 그 내용인 사실성과는 별개의 문제이다. 이에 이하에서는 LLM 시스템에서 가장 중요한 실패 유형인 **Hallucination**을 중심으로, 이 문제가 왜 발생하는지, 그리고 왜 모델 개선이 아니라 시스템 설계 문제로 다뤄져야 하는지에 대해 정리해 보고자 한다.

---

# 2. Hallucination의 발생 이유

Hallucination은 LLM이 사실과 무관하거나 근거 없는 내용을 생성하는 현상을 의미한다. 이는 모델이 거짓을 말하려는 의도를 가지고 있기 때문이 아니라, **LLM의 목적 함수 자체에서 유발되는 문제**이다.

언어 모델은 본질적으로 다음 확률을 최대화하는 방향으로 활성화된다.

즉, 모델은 *가장 그럴듯해 보이는 다음 토큰*을 생성할 뿐, 생성한 문장이 사실인지 아닌지에 대해서는 검증하지 않는다. 입력에 충분한 정보가 없거나 애매한 질문이 주어진 경우에도 모델은 무응답을 하기보다는 확률적으로 자연스러운 답변을 생성한다.

이러한 구조적 특징을 고려할 때 Hallucination은 예외가 아니라 **필연적인 현상**에 가깝다고 볼 수 있다.

---

# 3. Hallucination의 주요 유형

실제 시스템에서 관찰되는 Hallucination은 단일 현상이 아니라 서로 다른 원인과 형태를 가지며, 대체로 다음 세 가지 유형으로 나뉜다.

## ① 사실적 Hallucination (Factual)

존재하지 않는 논문, 인물, 법 조항을 만들어내거나 잘못된 수치와 날짜를 제시하는 경우이다.  
이는 주로 **지식 공백이나 검색 실패** 상황에서 발생한다.

## ② 추론 기반 Hallucination (Reasoning)

전제 자체가 틀린 상태에서 논리적으로 그럴듯한 결론을 도출하는 경우이다.  
이는 주로 **Chain-of-Thought나 Agent 구조에서 중간 오류가 누적**되며 자주 나타난다.

## ③ 지시 불일치 Hallucination (Instructional)

질문의 범위를 벗어나거나, 명시되지 않은 조건을 임의로 가정해 답변하는 경우이다.  
이는 **프롬프트 해석 실패나 과도한 일반화**에서 발생한다.

이 세 유형은 서로 다른 원인을 가지지만, 공통적으로 **모델이 불확실성을 다루는 방식의 한계**를 드러낸다.

---

# 4. Hallucination의 완전 제거가 불가능한 이유

Hallucination을 완전히 제거하기 어려운 이유는 언어 모델의 구조에서 비롯된다.

- 언어 모델은 **확률 모델**이지 검증 시스템이 아니다.  
- 출력 공간에 **‘모르겠다’**는 선택지가 명시적으로 존재하지 않는다.  
- 학습 데이터 자체에 **노이즈와 불확실성**이 존재한다.  

이를 고려할 때 Hallucination은 모델의 오류라기보다, **언어 모델이라는 접근 방식이 가지는 근본적인 제약**이라고 할 수 있다.

따라서 이 문제를 해결하는 방법은 *더 좋은 모델을 찾는 것*이 아니라, **어떻게 사용할 것인가를 설계하는 방향**으로 나아가야 한다.

---

# 5. Hallucination의 완화

## 5.1 입력 제어 (RAG & Prompt)

Hallucination 완화의 첫 번째 축은 **입력 제어**이다.

- RAG를 통해 근거 문서 제공  
- 출처 기반 답변 요구  
- “모르면 모른다”는 응답을 허용하는 명시적 제약  

이는 모델의 추론 방식을 바꾸는 것이 아니라, **조건부 분포를 더 안전한 영역으로 이동**시키는 방법이다.

## 5.2 출력 제어 (Verification & Filtering)

두 번째 축은 **출력 이후의 제어**이다.

- 근거 문서와 답변 간 일치성 검증  
- 다중 샘플링 후 합의(consensus) 사용  
- 규칙 기반 필터링  

이 접근은 LLM을 무조건 신뢰하지 않고, **출력을 검증 대상**으로 취급한다는 점에서 중요하다.

## 5.3 학습 기반 접근

이 외에도 학습 단계에서 Hallucination을 줄이려는 시도가 존재한다.

- 사실성 중심 Reward Model  
- 근거 기반 응답에 보상 부여  
- Refusal(거절) 응답 학습  

그러나 학습 기반 접근은 **비용이 크고 일반화가 어렵다**는 한계를 가진다.

---

# 6. 신뢰성(Reliability)의 개념

LLM 시스템에서의 신뢰성이란 단순히 정답을 맞히는 비율이 아니다. 이는 다음을 포함한다.

- 동일한 입력에 대해 **일관된 출력**을 내는지  
- 실패할 경우 **예측 가능한 방식으로 실패**하는지  
- 불확실성을 **과장하지 않고 적절히 표현**하는지  

즉, 신뢰성은 **평균 성능이 아니라 출력 품질의 분산을 관리하는 문제**이다.  
이 관점에서 Hallucination은 단일 오류가 아니라 **시스템 품질 관리의 실패**라고 할 수 있다.

---

# 7. 마무리

LLM 평가가 어려운 이유는 정답이 하나로 정의되지 않았을뿐더러, 태스크가 정형화되어 있지 않기 때문이다. BLEU나 ROUGE 같은 기존 지표는 LLM의 특성을 반영하지 못한다.

이에 최근에는 다음과 같은 **시스템 수준 평가 축**이 사용된다.

- Faithfulness  
- Helpfulness  
- Harmlessness  
- Calibration  

결과적으로 Hallucination과 신뢰성 문제는 **모델 성능 문제가 아니라 LLM 단독 사용에 따른 구조적 문제**라고 할 수 있다.

그렇기에 입력을 통제하고, 출력을 검증하며, 실패를 관리하는 **시스템 설계**가 없으면 이 문제는 반복된다.

LLM의 불완전성을 이해하고, Hallucination을 *버그가 아니라 관리해야 할 위험 요소*로 보는 것이 현대 LLM 활용의 핵심 전환점이라고 할 수 있다.

---



