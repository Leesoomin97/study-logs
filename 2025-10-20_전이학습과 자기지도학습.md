# 전이학습과 자기지도학습 (Transfer Learning & Self-Supervised Learning)

---

## 1. 머리말

딥러닝 모델의 성능은 데이터의 양과 다양성에 크게 의존한다. 데이터 증강이 데이터의 양을 인위적으로 늘리는 기술이라면 전이학습과 자기지도학습은 데이터의 양을 늘리지 않고 모델 성능을 높이는 방법이다.

이하에서는 전이학습과 자기지도학습의 개념과 원리, 장단점, 그리고 실제 활용 사례를 살펴보고자 한다.

---

## 2. 전이학습(Transfer Learning)

### (1) 개념
전이학습이란 이미 학습된 모델의 지식을 새로운 문제에 재활용하는 방법이다. 대규모 데이터셋으로 학습된 모델은 일반적인 패턴을 이미 익혔기 때문에, 소규모 데이터 환경에서도 효율적으로 성능을 낼 수 있다.

---

### (2) 주요 방식

| 구분 | 설명 | 예시 |
|------|------|------|
| Feature Extractor | 사전학습 모델의 하위 계층만 사용하고 상위 계층은 새로 학습한다. | ResNet feature + custom classifier |
| Fine-Tuning | 전체 모델을 낮은 학습률로 재학습하여 새로운 태스크에 적응시킨다. | BERT fine-tuning for sentiment classification |
| Partial Freezing | 일부 계층은 동결하고 나머지만 학습시킨다. | EfficientNet 중간 계층만 고정 |

---

### (3) 장점
- 데이터가 적어도 성능이 우수하다.  
- 학습 속도가 빠르고 자원 소모가 적다.  
- 기존 지식을 재활용하므로 일반화 능력이 높다.

---

### (4) 단점
- 사전학습 데이터와 목표 데이터의 도메인이 다르면 성능이 낮아질 수 있다.  
- Fine-tuning 과정에서 잘못된 학습률 설정 시 기존 가중치가 손상될 수 있다.  
- 모델 구조가 복잡하면 일부 계층만 조정해도 과적합이 발생할 수 있다.

---

### (5) 예시 코드(PyTorch)

```python
import torch
import torch.nn as nn
from torchvision import models

model = models.resnet50(pretrained=True)
num_ftrs = model.fc.in_features
model.fc = nn.Linear(num_ftrs, 10)

for name, param in model.named_parameters():
    if "layer4" not in name:
        param.requires_grad = False
```

---

## 3. 자기지도학습(Self-Supervised Learning)

### (1) 개념
자기지도학습이란 라벨이 없는 데이터에서 스스로 감독 신호를 만들어 학습하는 방법이다. 모델이 데이터를 통해 스스로 문제를 만들고 해결하면서 일반적 표현(representation)을 학습한다.

즉, 인간의 개입 없이 스스로 감독자(Supervisior)가 되는 학습 방식이다.

---

### (2) 주요 구조

① **Pretext Task(사전 과제)**  
인위적으로 생성된 가짜 라벨을 예측하는 과제이다.  
예: 이미지 일부 가리기, 문장 일부 마스크 등  

② **Downstream Task(실제 과제)**  
사전 과제로 학습된 표현을 기반으로 실제 태스크를 수행한다.  
예: 분류, 감정분석, 객체 인식 등  

---

### (3) 대표 알고리즘

| 분야 | 알고리즘 | 핵심 아이디어 |
|------|-----------|----------------|
| 이미지 | SimCLR, MoCo, BYOL, DINO | 두 augmented view 간 feature를 가깝게 만든다(Contrastive Learning) |
| 텍스트 | BERT(Masked LM), GPT(Next Token Prediction) | 문맥 기반 단어 토큰 예측을 수행한다. |
| 오디오 | Wav2Vec 2.0 | 마스크된 오디오 구간을 복원한다. |
| 비전 + 언어 | CLIP | 이미지와 텍스트 쌍의 유사도를 학습한다. |

---

### (4) 장점
- 라벨 없이도 대규모 데이터를 학습할 수 있다.  
- 전이학습보다 더 범용적인 표현을 학습한다.  
- 데이터 효율성과 일반화 능력이 높다.

---

### (5) 단점
- Contrastive Loss 학습 시 대규모 batch 연산이 필요하여 하드웨어 요구가 높다.  
- Pretext Task의 설계가 부적절하면 학습 품질이 떨어질 수 있다.  
- Downstream Task에 맞게 세밀한 Fine-tuning이 필수적이다.

---

### (6) 예시 코드(SimCLR 스타일)

```python
# 개념적 예시
z_i = encoder(augment(x))
z_j = encoder(augment(x))
loss = contrastive_loss(z_i, z_j)
```

---

## 4. 실제 활용 사례

| 분야 | 적용 예시 | 효과 |
|------|------------|------|
| 이미지 인식 | ResNet50을 ImageNet 사전학습 후 Fine-Tuning | 학습 데이터 10%로도 기존 성능 유지 |
| 자연어 처리 | BERT fine-tuning으로 감정 분류 | 기존 RNN 대비 정확도 5~10% 향상 |
| 음성 인식 | Wav2Vec 2.0 | 라벨 10%만으로 전체 성능의 95% 달성 |
| 멀티모달 | CLIP(OpenAI) | 이미지와 텍스트를 동시에 이해하는 모델 구축 |

---

## 5. 세 접근법 비교

| 구분 | 라벨 필요 여부 | 데이터 사용 방식 | 대표 예시 | 장점 | 단점 |
|------|----------------|------------------|------------|------|------|
| 데이터 증강 | 필요 | 기존 데이터 변형 | CutMix, AutoAugment | 일반화 성능 향상 | 변형이 과하면 의미 훼손 가능 |
| 전이학습 | 필요 | 사전학습 모델 재활용 | ResNet, BERT | 소량 데이터에서도 고성능 | 도메인 차이 시 효과 제한 |
| 자기지도학습 | 불필요 | 데이터 자체에서 감독 신호 생성 | SimCLR, CLIP | 비라벨 데이터 활용 가능 | 학습 복잡도와 자원 소모가 큼 |

---

## 6. 마무리

AI 모델 개발의 패러다임은 점점 라벨 없이 배우고, 적은 데이터로도 강인하게 학습하는 방향으로 이동하고 있다. 이에 기존의 중심이 되던 데이터 증강이 아닌 전이학습과 자기지도학습에 대한 관심이 커지고 있다.

이에 우리는 데이터가 적더라도 그 속에 숨어 있는 패턴을 어떻게 끌어낼 것인가에 집중해야 한다.
