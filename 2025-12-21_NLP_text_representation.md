# 1. 머리글

자연어 처리 모델은 텍스트를 직접 이해하지 못한다. 모델에 문장을 입력하기 위해서는 단어를 수치적 표현(vector)으로 변환해야 하며, 이를 텍스트 표현(Text Representation) 문제라고 한다.

텍스트 표현 방식은 단순한 전처리 단계가 아니라, 모델이 언어를 어떤 관점에서 해석할지를 결정한다.

이번 글에서는 Bag-of-Words와 TF-IDF 같은 초기 표현 방식부터, 의미 정보를 반영하기 위해 등장한 Word Embedding까지의 흐름을 정리한다.

---

# 2. Bag-of-Words (BoW)

Bag-of-Words는 문장을 단어의 등장 여부 또는 등장 횟수로 표현하는 가장 단순한 방식이다. 단어의 순서나 문장 구조는 고려하지 않으며, 각 문장은 사전에 정의된 어휘 집합에 대한 벡터로 변환된다.

예를 들어 다음 두 문장을 생각해 보자.

I like this movie
This movie is not good


BoW 방식에서는 단어의 순서를 무시하기 때문에, 두 문장이 공유하는 단어(this, movie)가 많다면 의미와 관계없이 유사한 벡터로 표현될 수 있다. 즉, 부정 표현이나 문장 구조에 따른 의미 차이를 반영하지 못한다.

이러한 특성 때문에 BoW는 구현이 단순하고 해석이 쉬운 장점이 있지만, 문맥과 의미 정보를 다루기에는 구조적인 한계를 가진다. 초기 문서 분류나 키워드 기반 태스크에서는 효과적이었으나, 언어 이해가 필요한 문제에는 적합하지 않다.

---

# 3. TF-IDF

TF-IDF는 Bag-of-Words의 단점을 보완하기 위해 등장한 가중치 기반 표현 방식이다. 단순히 단어가 몇 번 등장했는지가 아니라, 해당 문서에서 얼마나 중요한 단어인지를 반영한다.

예를 들어 영화 리뷰 데이터에서 movie라는 단어는 대부분의 문서에 등장한다. 반면 boring, excellent와 같은 단어는 특정 문서를 더 잘 설명한다. TF-IDF는 이러한 단어에 더 높은 가중치를 부여하여 문서 간 변별력을 높인다.

이 방식은 검색 시스템이나 문서 분류에서 BoW보다 안정적인 성능을 보인다. 그러나 TF-IDF 역시 단어를 독립적인 기호로 취급하기 때문에, 단어 간 의미적 유사성이나 문맥 정보는 반영하지 못한다는 한계를 가진다.

---

# 4. Count 기반 표현 방식의 한계

BoW와 TF-IDF는 모두 단어의 등장 빈도에 기반하며, 다음과 같은 공통적인 문제를 가진다.

- 의미적으로 유사한 단어를 전혀 다른 벡터로 표현  
- 어휘 수 증가에 따른 고차원 희소 벡터(Sparse Vector) 문제  
- 학습 데이터에 없는 단어(OOV) 처리 불가  
- 문맥 정보 부재  

이로 인해 모델은 언어의 의미를 학습하기보다는, 표면적인 단어 패턴에 의존하게 된다.

---

# 5. Word Embedding의 등장

이러한 한계를 등장하기 위해 등장한 개념이 Word Embedding이다.

Word Embedding은 단어를 고정 차원의 밀집 벡터(Dense Vector)로 표현하며, 단어가 사용되는 문맥을 기반으로 학습된다. 이는 단어를 단순한 기호가 아니라, 의미를 가진 객체로 다루려는 시도라고 볼 수 있다.

Count 기반 표현에서는 각 단어가 서로 독립적인 차원으로 취급되었지만, Word Embedding에서는 단어 간의 관계가 벡터 공간에 직접 반영된다. 의미적으로 유사한 단어는 벡터 공간에서도 가까운 위치에 배치되며, 반대로 의미가 다른 단어는 멀리 떨어지게 된다.

이러한 표현 방식 덕분에 단어 간 유사도 계산, 군집화, 의미적 관계 분석이 가능해졌다. 텍스트를 단순히 빈도의 집합이 아니라, 의미 공간 상의 분포로 다룰 수 있게 되었다는 점에서 NLP 표현 방식의 중요한 전환점이라 할 수 있다.

---

# 6. Word2Vec

Word2Vec은 Word Embedding을 학습하기 위한 대표적인 방법이다. 주변 단어와의 관계를 이용하여 단어 의미를 학습하며, CBOW와 Skip-gram이라는 두 가지 구조를 가진다.

CBOW는 주변 단어들을 입력으로 사용해 중심 단어를 예측하고, Skip-gram은 중심 단어를 이용해 주변 단어를 예측한다. 구조는 다르지만, 두 방식 모두 단어가 등장하는 문맥 정보를 학습 신호로 활용한다는 공통점을 가진다.

Word2Vec의 중요한 특징은 학습 과정에서 단어 간 공기(co-occurrence) 정보를 직접 세지 않는다는 점이다. 대신 예측 문제를 통해 의미를 간접적으로 학습하며, 이로 인해 계산 효율성과 표현력이 크게 향상되었다.

이 접근 방식은 기존 Count 기반 표현보다 훨씬 적은 차원으로도 의미 정보를 담을 수 있었고, 이후 등장하는 다양한 NLP 모델과 임베딩 기법의 기반이 되었다.

---

# 7. 정적 임베딩의 한계

Word2Vec과 같은 임베딩 방식은 정적 임베딩(static embedding)이라고 불린다. 각 단어가 학습 과정에서 하나의 고정된 벡터만을 가지기 때문이다.

이러한 방식은 단어의 평균적인 의미를 표현하는 데에는 효과적이지만, 문맥에 따라 의미가 달라지는 단어를 구분하지 못한다는 한계를 가진다. 동일한 단어가 서로 다른 의미로 사용되더라도, 항상 같은 벡터로 처리된다.

이 문제는 실제 NLP 태스크에서 중요한 제약으로 작용한다. 문장 단위 의미 이해나 복잡한 문맥 해석이 필요한 경우, 정적 임베딩만으로는 충분한 표현력을 확보하기 어렵다.

이러한 한계는 이후 문맥 기반 임베딩(Contexttual Embedding)과 Transformer 계열 모델이 등장하는 직접적인 배경이 되었다.

---

# 8. 표현 방식 비교

| 구분 | BoW | TF-IDF | Word Embedding |
|---|---|---|---|
| 표현 기준 | 등장 빈도 | 가중치 빈도 | 문맥 기반 의미 |
| 벡터 형태 | Sparse | Sparse | Dense |
| 문맥 반영 | X | X | O |
| 의미 유사도 | X | X | O |
| 주요 한계 | 의미 손실 | 문맥 부재 | 문맥 변화 반영 불가 |

---

# 9. 마무리

Bag-of-Words와 TF-IDF는 단순하지만 해석이 쉬운 텍스트 표현 방식이다. Word Embedding은 단어의 의미를 학습함으로써, NLP 모델이 언어를 보다 구조적으로 다룰 수 있도록 만들었다.

텍스트 표현 방식의 이러한 변화는 이후 Sequence Modeling과 Transformer 구조로 자연스럽게 이어진다. 이에 다음 글에서는 순차 데이터를 처리하기 위한 RNN 기반 모델과 Sequence Modeling의 한계를 살펴보고자 한다.

---
