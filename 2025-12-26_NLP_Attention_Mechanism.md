# 1. 머리글

RNN은 자연어를 순차적으로 처리할 수 있게 만들었지만, 모든 과거 정보를 하나의 hidden state에 압축해야 한다는 구조적 제약을 가지고 있었다. 문장이 길어질수록 초기 정보는 점점 희석되었고, 이는 번역, 요약과 같은 태스크에서 명확한 성능 한계로 드러났다.

Attention 메커니즘은 이 문제를 기억의 용량 문제가 아니라 정보 선택의 문제로 재정의하며 등장했다. 즉, 과거 정보를 하나로 요약하는 대신, 현재 출력에 필요한 정보가 무엇인지를 매 시점마다 계산하자는 접근이다.

이번 글에서는 Attention이 기존 RNN 기반 접근과 계산 구조 차원에서 무엇이 달라졌는지, 그리고 이 구조가 이후 Transformer로 어떻게 확장될 수 있었는지를 중심으로 살펴보고자 한다.

---

# 2. RNN 기반 문맥 처리의 구조적 한계

RNN 계열 모델에서 시점 t의 hidden state인 `h_t`는 다음과 같이 계산된다.

- 
h_t = f(h_{t-1}, x_t)


이 구조는 이전 정보가 누적된 하나의 벡터를 통해 문맥을 전달한다. 문제는 이 벡터가 모든 과거 정보를 동일한 비중으로 압축해야 한다는 점이다.

그 결과,

- 현재 예측에 필요한 정보와 불필요한 정보가 구분되지 않고  
- 문장이 길어질수록 초기 입력의 영향력은 감소하며  
- 어디에 집중해야 하는지를 모델이 명시적으로 학습할 수 없다  

즉, RNN은 문맥을 선택하지 못하고 보관만 하는 구조이다.

---

# 3. Attention의 문제 설정: Alignment

Attention은 문맥 처리 문제를 **“현재 출력 시점에서, 입력 시퀀스의 어떤 위치가 가장 관련 있는가?”**의 문제로 다시 정의한다.

이 질문은 alignment 문제로 이어진다. 즉, 출력 시점 `t`와 입력 시점 `i` 사이의 관련성을 수치로 계산하는 것이다.

이를 위해, Attention은 다음과 같이 역할을 구분한다.

- 출력 쪽 상태를 **Query**  
- 입력 쪽 상태를 **Key / Value**  

이 구분이 이후 모든 Attention 구조의 출발점이 된다.

---

# 4. Attention의 계산 구조

Attention은 다음 세 단계로 구성된다.

## (1) Alignment score 계산

출력 시점의 query인 `q_t`와 입력 시점의 key인 `k_i` 사이의 score를 계산한다. 가장 단순한 형태는 dot-product이다.

- e_{t,i} = q_t^T k_i

이 score는 현재 출력이 입력의 어느 위치를 참고해야 하는지를 나타낸다.

## (2) Attention weight 계산

score를 softmax로 정규화하여 확률 분포로 변환한다.

- α_{t,i} = exp(e_{t,i}) / ∑j exp(e{t,j})

이 값은 각 입력 위치가 현재 출력에 얼마나 기여하는지를 의미한다.

## (3) Context vector 생성

attention weight를 value에 적용해 가중합을 계산한다.

- c_t = ∑i α{t,i} v_i

이렇게 생성된 `c_t`가 현재 시점의 문맥 요약(context vector)이 된다. 중요한 점은 출력 시점마다 서로 다른 context vector가 생성된다는 것이다.

---

# 5. RNN + Attention 구조의 의미

초기의 Attention은 RNN을 대체하지 않았다. 대신 RNN의 출력 위에 선택 메커니즘을 추가했다.

- **RNN**: 시퀀스의 순서 정보 처리  
- **Attention**: 문맥 정보 선택 및 재조합  

이 구조 덕분에 모델은 문장 전체를 요약한 벡터가 아니라, 출력 단어마다 다른 문맥을 참조할 수 있게 되었다.

이는 번역, 요약, 질의응답 태스크에서 RNN 단독 구조 대비 결정적인 성능 향상을 가져왔다.

---

# 6. Attention이 해결한 것과 남은 한계

Attention은 다음 문제를 구조적으로 완화했다.

- 장기 의존성 문제  
- 문맥 정보 희석  
- 고정된 문맥 벡터 병목  

그러나 여전히 한계가 남아있다.

- RNN의 순차 처리 구조는 그대로 유지  
- 병렬화 불가능  
- 긴 시퀀스에서 계산 비용 증가  

이 한계는 자연스럽게 **“Attention만으로 시퀀스를 처리할 수 없을까?”**라는 질문으로 이어진다.

---

# 7. 마무리

Attention 메커니즘은 자연어 처리에서 문맥 처리 방식을 근본적으로 바꾼 전환점이다. 이는 모든 과거 정보를 동일하게 취급하던 구조에서 벗어나, 현재 출력에 필요한 정보를 계산적으로 선택할 수 있게 만들었다는 점에서 의미가 크다.

이 개념은 이후 Self-Attention과 Transformer 구조의 핵심 구성 요소로 확장되며, 자연어 처리 모델의 패러다임을 완전히 바꾸게 된다.

---




