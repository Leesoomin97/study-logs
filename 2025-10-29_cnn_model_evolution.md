# CNN 구조 발전 과정 분석 

## 1. 머리말

Convolutional Neural Network(CNN)은 이미지의 공간적 특징을 학습하기 위한 핵심 신경망 구조이다.  
기본 CNN은 합성곱(Convolution), 활성화 함수(Activation), 풀링(Pooling), 완전연결층(Fully Connected)으로 구성되며,  
이 구조만으로도 객체 분류와 특징 추출이 가능하다.  

하지만 CNN의 성능은 단순히 깊이를 늘린다고 향상되지 않는다.  
CNN의 깊이가 깊어질수록 그 학습 안정성, 연산 효율성, 확장성 등 다양한 기술적 문제가 병존한다.  

따라서 CNN의 발전사는 단순한 모델 이름의 나열이 아니라,  
딥러닝의 구조적 한계를 극복해온 기술적 해결 과정을 의미한다.  

그렇기에 이번 글에서는  
① 각 세대의 모델이 어떤 문제를 인식하고 해결했는지 파악하고,  
② 구조적 개선이 성능, 효율, 안정성에 어떤 영향을 미쳤는지 이해하며,  
③ 최신 모델 설계(Vision Transformer, ConvNeXt 등)에 적용 가능한 구조 설계 원리를 학습하고자 한다.  

---

## 2. AlexNet (2012)

(1) **문제**  
기존 CNN은 소규모 데이터셋(MNIST 등)에서는 작동했지만,  
대규모 데이터셋(ImageNet)을 학습하기에는 연산 자원이 부족하고 과적합이 빈번했다.  

(2) **구조적 개선**  
GPU 병렬 연산을 도입하여 대규모 학습을 가능하게 했다.  
비선형 활성화 함수 ReLU를 적용해 학습 속도를 향상시켰다.  
Dropout으로 과적합을 완화하고, Local Response Normalization(LRN)으로 활성값 분포를 정규화했다.  
이러한 설계는 5개의 합성곱층(Conv)과 3개의 완전연결층(FC)으로 구성된 기본 CNN 아키텍처를 기반으로 한다.  

(3) **기술적 의의**  
GPU를 활용한 대규모 데이터 학습의 가능성을 입증한 첫 CNN이다.  
이후 모든 딥러닝 학습 인프라가 GPU 중심으로 구축되는 계기가 되었다.  

---

## 3. VGGNet (2014)

(1) **문제**  
AlexNet의 필터 크기(11x11, 5x5)는 일관성이 없어 구조 설계가 복잡했고,  
네트워크 확장 시 재현성이 낮았다.  

(2) **구조적 개선**  
모든 합성곱층을 3x3 필터로 통일해 구조를 단순화했다.  
작은 필터를 여러 층으로 반복 적용하여 수용 영역(receptive field)를 확대했다.  
각 블록에 ReLU + MaxPooling을 결합하여 비선형성 확보와 차원 축소를 병행했다.  

(3) **기술적 의의**  
규칙적이고 단순한 구조가 재현성과 확장성을 향상시킨다는 점을 검증했다.  
이후 대부분의 CNN 백본(backbone) 설계가 VGG 구조를 변형한 형태로 발전했다.  

---

## 4. GoogleNet (Inception, 2014)

(1) **문제**  
VGGNet은 높은 정확도를 달성했지만,  
파라미터 수(약 138M)와 연산량이 과도해 효율성이 낮은 문제가 존재했다.  

(2) **구조적 개선**  
Inception 모듈을 도입하여 1x1, 3x3, 5x5 필터를 병렬로 적용했다.  
1x1 Convolution으로 채널 차원을 축소해 연산량을 크게 줄였다.  
Global Average Pooling으로 FC층을 제거하여 파라미터 수를 최소화했다.  

(3) **기술적 의의**  
병렬 연산과 차원 축소를 결합해 정확도 대비 파라미터 효율을 대폭 개선했다.  
효율 중심 CNN 설계의 기초를 마련했으며, MobileNet, EfficientNet 등의 전신이 되었다.  

---

## 5. ResNet (2015)

(1) **문제**  
네트워크가 깊어질수록 gradient 소실이 심화되어,  
100층 이상의 모델부트는 학습이 불가능해졌다.  

(2) **구조적 개선**  
Residual Block을 도입하여 입력 x를 출력 F(x)와 더하는 Skip Connection 구조를 제안했다.  
Gradient가 입력 경로를 통해 직접 전파되어 깊은 네트워크에서도 학습 안정성을 확보했다.  
Batch Normalization으로 분포 변화를 정규화하고, ReLU로 비선형성을 유지했다.  

(3) **기술적 의의**  
Residual Learning 개념을 확립해 깊은 신경망 학습의 한계를 해결했다.  
이후 대부분의 딥러닝 모델(ResNeXt, Transformer 등)에 기본 구조로 채택되었다.  

---

## 6. EfficientNet (2019)

(1) **문제**  
기존 모델들은 깊이(depth), 너비(width), 해상도(resolution)를 독립적으로 조정했으며,  
그 결과 확장 시 연산 효율이 급격히 떨어졌다.  

(2) **구조적 개선**  
Compound Scaling 기법을 적용해 세 축을 일정 비율로 동시에 확장했다.  
MBConv (Mobile Inverted Bottleneck)을 기본 블록으로 사용해 연산량을 최소화했다.  
Squeeze-and-Excitation(SE) 모듈을 추가해 채널 중요도를 학습하도록 설계했다.  

(3) **기술적 의의**  
모델 크기(B0~B7)와 연산 리소스 간의 균형적 확장을 실현했다.  
효율성과 정확도를 동시에 달성하며 EfficientNetV2, ConvNeXt 등으로 발전했다.  

---

## 7. 종합 비교

| 모델 | 핵심 개선 | 해결한 문제 | 파라미터 수 | 발표 연도 |
|------|-------------|--------------|---------------|-----------|
| AlexNet | GPU 학습, ReLU | 대규모 학습 불가 | 60M | 2012 |
| VGGNet | 3x3 규칙적 구조 | 복잡한 설계, 재현성 부족 | 138M | 2014 |
| GoogLeNet | Inception 모듈 | 연산 비효율 | 6.8M | 2014 |
| ResNet | Skip Connection | Gradient 소실 | 25M | 2015 |
| EfficientNet | Compound Scaling | 비효율적 확장 | 5M (B0 기준) | 2019 |

---

## 8. 마무리

CNN의 발전은 단순한 모델 깊이 경쟁이 아니라,  
연산 효율성과 학습 안정성, 확장성 간의 균형을 찾는 구조적 진화 과정이었다.  

- AlexNet: GPU를 통한 대규모 학습 실현  
- VGGNet: 구조의 단순화와 일관성 확보  
- GOogLeNet: 병렬 처리 및 차원 축소로 연산 효율화  
- ResNet: Gradient 흐름 안정화  
- EfficientNet: 균형적 확장 전략 정립  

이러한 발전의 흐름은 오늘날 Vision Transformer(ViT), ConvNeXt, Diffusion Model 등에 직접적인 영향을 미쳤다.  

따라서 CNN의 발전사를 이해하는 것은  
딥러닝 모델의 구조적 문제를 정의하고 해결하는 방법론을 학습하는 과정이라 할 수 있다.  

---

