# 1. 머리글

자연어 처리(NLP) 모델은 텍스트를 그대로 입력으로 사용할 수 없다. 모든 NLP 모델은 텍스트를 수치 형태로 변환한 뒤 학습과 추론을 수행하며, 이 과정의 출발점이 텍스트 전처리(Text Processing)이다.

텍스트 전처리는 단순한 정제 작업이 아니라, 모델이 어떤 단위로 언어를 인식할지를 결정하는 단계다. 본 글에서는 NLP에서 전처리가 왜 필요한지 살펴보고, 토큰화(Tokenization)를 중심으로 전처리 방식의 흐름을 정리해 보고자 한다.

---

# 2. 텍스트 전처리의 목적

텍스트 전처리의 주요 목적은 다음과 같다.

- 모델 입력 형식 통일  
- 불필요한 노이즈 제거  
- 어휘 공간 축소  
- 학습 안정성 및 일반화 성능 향상  

전처리는 데이터 품질을 개선하지만, 과도할 경우 의미 손실을 유발할 수 있다.

---

# 3. 전통적인 텍스트 전처리 기법

전통적인 NLP에서는 정규화, 불용어 제거, stemming, lemmatization이 주로 사용되었다. 이 기법들은 공통적으로 어휘 수를 줄이고 통계적 모델의 학습을 돕는 역할을 한다.

## (1) 불용어 제거(Stopword Removal)

불용어 제거는 문장 내에서 의미 기여도가 낮다고 판단되는 단어를 제거하는 방식이다. BoW나 TF-IDF 기반ㄴ 모델에서는 문서 간 변별력을 높이는 데 효과적이었다.

그러나 딥러닝 기반 NLP에서는 문맥 전체를 학습하기 때문에, 불용어 제거가 의미 정보 손실로 이어질 수 있다. 특히 부정 표현이나 문장 구조를 나타내는 단어는 의미 해석에 중요한 역할을 한다.

→ 최근에는 기본 전처리 단계로 사용하지 않고, 태스크에 따라 선택적으로 적용하는 경우가 많다.

## (2) Stemming과 Lemmatization

Stemming과 Lemmatization은 단어 형태 변화를 줄이기 위한 기법이다.

- **Stemming**: 규칙 기반으로 어미를 제거  
- **Lemmatization**: 사전 기반으로 단어를 원형으로 변환  

두 방식 모두 어휘 수를 줄이는 데 효과적이지만, 문맥을 고려하지 않기 때문에 의미 왜곡이 발생할 수 있다. 이로 인해 최신 NLP 파이프라인에서는 사용 빈도가 감소했다.

---

# 4. 토큰화(Tokenization)

토큰화는 텍스트를 모델이 처리할 수 있는 최소 단위(Token)로 분해하는 과정이다. 토큰 단위의 선택은 모델 성능과 직접적으로 연결된다.

## (1) Word-level Tokenization

초기 NLP 시스템에서 사용되던 방식으로, 단어 단위로 텍스트를 분리한다. 하지만 다음과 같은 한계를 가진다.

- OOV(Out of Vocabulary) 문제  
- 희귀 단어 처리 어려움  
- 어휘 크기 증가  

## (2) Subword Tokenization

Subwork 토큰화는 단어를 더 작은 단위로 분해하여 표현하는 방식이다. 이를 통해 새로운 단어도 기존 토큰 조합으로 처리할 수 있다.

대표적인 방식은 다음과 같다.

- **BPE**: 빈도 기반 병합 (GPT 계열)  
- **WordPiece**: 확률 기반 병합 (BERT 계열)  
- **Unigram LM**: 후보 토큰 중 최적 조합 선택 (SentencePiece)  

---

# 5. SentencePiece와 한국어 전처리

SentencePiece는 공백을 기준으로 하지 않는 Subword 토큰화 방식으로, 원문 텍스트를 그대로 입력으로 받아 토큰화를 수행한다. 이는 띄어쓰기 규칙이 일관되지 않거나 형태 변화가 많은 언어에서도 안정적으로 적용할 수 있다는 장점이 있다.

한국어는 조사와 어미변화가 잦고, 동일한 의미라도 다양한 표기로 나타난다. 이러한 특성 때문에 단어 단위 토큰화는 어휘 분산과 희귀 단어 문제를 쉽게 유발한다. SentencePiece 기반 토큰화는 단어를 더 작은 서브 워드 단위로 분해함으로써 이러한 문제를 완화하고, 띄어쓰기 오류에 대한 민감도를 낮춘다.

이로 인해 한국어를 포함한 다국어 NLP 환경에서는 SentencePiece 또는 유사한 Subwork 토큰화 방식이 표준적으로 사용된다.

---

# 6. 전처리 전략 선택 시 고려 사항

텍스트 전처리는 모델과 태스크에 따라 달라져야 한다.

- 분류 vs 생성 태스크  
- 데이터 규모  
- 언어 특성  
- 사용 모델(BERT, GPT 등)  

모든 전처리를 자동으로 적용하기보다는, 목적에 맞는 최소한의 전처리가 중요하다.

---

# 7. 마무리

이번 글에서는 텍스트 전처리와 토큰화의 역할에 대해 정리해 보았다. 다음 글에서는 단어를 수치로 표현하는 방법인 Bag-of-Words, TF-IDF, Word Embedding의 흐름을 살펴보고자 한다.

---
