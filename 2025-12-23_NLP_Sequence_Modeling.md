# 1. 머리글

단어를 벡터로 바꾸는 것만으로는 자연어의 의미를 충분히 다룰 수 없다. 자연어에서 중요한 것은 어떤 단어가 등장했는가뿐 아니라, 어떤 순서로 등장했는가도 포함된다.

자연어는 대표적인 순차 데이터(sequence data)이며, 단어의 위치와 앞뒤 관계에 따라 의미가 달라진다. 특히 부정 표현, 강조 표현, 조건절과 같은 요소는 문장 내 위치에 크게 의존한다. 이러한 특성 때문에 자연어 처리에서는 단어를 독립적으로 다루는 방식보다, 시간적 흐름과 문맥 누적을 고려한 모델링이 필요하다.

이러한 요구를 배경으로 등장한 모델이 RNN(Recurrent Neural Network)이다. RNN은 이전 시점의 정보를 현재 시점으로 전달함으로써, 순서 정보를 자연스럽게 반영하려는 시도를 담고 있다. 이번 글에서는 RNN이 어떤 문제를 해결하려 했는지, 그리고 실제 적용 과정에서 어떤 구조적 한계에 부딪히게 되었는지를 중심으로 살펴보고자 한다.

---

# 2. Sequence Modeling이란 무엇인가

Sequence Modeling은 입력 데이터가 순서를 가지는 경우, 이 순서 정보를 고려하여 모델링하는 문제를 의미한다.

자연어 처리에서는 다음과 같은 특징 때문에 Sequence Modeling이 필수적이다.

- 단어의 순서가 의미에 직접적인 영향을 줌  
- 앞에 나온 단어가 뒤의 해석에 영향을 미침  
- 문장 길이가 가변적임  

예를 들어 다음 두 문장은 단어 구성은 같지만 의미가 다르다.

- I like this movie
- I do not like this movie


이 차이를 반영하려면, 단어를 독립적으로 처리하는 방식으로는 부족하다.

---

# 3. RNN의 기본 구조

RNN은 이전 시점의 정보를 현재 시점으로 전달하는 구조를 가진다. 각 시점에서의 입력과 이전 hidden state를 함께 사용해 현재 상태를 계산한다.

핵심 아이디어는 다음과 같다.

- 이전 단어의 정보가 hidden state에 저장됨  
- 이 hidden state가 다음 단어 처리에 사용됨  
- 이를 통해 문맥 정보를 누적할 수 있음  

이 구조 덕분에 RNN은 문장을 왼쪽에서 오른쪽으로 읽으며 처리할 수 있게 되었다.

---

# 4. RNN이 해결하려 했던 문제

RNN은 기존의 Bag-of-Words나 Word Embedding 기반 접근에서 다루지 못했던 문제를 해결하려 했다.

- 단어 순서 정보 반영  
- 문장 길이에 상관없는 처리  
- 이전 단어에 따른 의미 변화 반영  

예를 들어, 문장 후반에 등장하는 부정 표현이 앞선 긍정 표현의 의미를 뒤집는 경우를 모델이 인식할 수 있게 된다.

이로 인해 RNN은 번역, 언어 모델링, 음성 인식 등 다양한 시퀀스 태스크에 적용되었다.

---

# 5. RNN의 구조적 한계

RNN은 개념적으로 자연어 처리에 잘 맞는 구조였지만, 실제 학습 과정에서 여러 문제가 드러났다.

## (1) 장기 의존성 문제(Long-Term Dependency)

문장이 길어질수록, 초반에 등장한 정보가 뒤로 전달되기 어려워진다. 이로 인해 중요한 단어가 문장 끝에서는 거의 영향을 주지 못하는 문제가 발생한다.

## (2) Gradient Vanishing/Exploding

RNN은 시간 축을 따라 반복적으로 같은 연산을 수행한다. 이 과정에서 역전파가 진행되면, gradient가 지나치게 작아지거나 커지는 문제가 발생한다.

- **Gradient Vanishing**: 학습이 거의 진행되지 않음  
- **Gradient Exploding**: 학습 불안정  

이는 RNN 학습을 매우 어렵게 만드는 주요 원인이다.

## (3) 순차 처리로 인한 비효율성

RNN은 이전 시점의 계산이 끝나야 다음 시점으로 넘어갈 수 있다. 이 때문에 병렬 처리가 어렵고, 긴 문장을 처리할수록 학습 시간이 급격히 증가한다.

대규모 데이터와 긴 문맥을 다뤄야 하는 현대 NLP 환경에서는 치명적인 단점이 된다.

---

# 6. LSTM과 GRU의 등장(개선 시도)

RNN의 한계를 해결하기 위해 LSTM과 GRU가 등장했다. 이들은 gating mechanism을 통해 정보의 흐름을 제어한다.

- 중요한 정보는 오래 유지  
- 불필요한 정보는 제거  

이를 통해 장기 의존성 문제를 일부 완화할 수 있었다. 하지만 구조가 복잡해지고, 순차 처리라는 근본적 한계는 여전히 남아 있었다.

---

# 7. 마무리

RNN은 자연어를 순차적으로 처리하기 위한 최초의 본격적인 딥러닝 접근 중 하나였다. 이전 단어의 정보를 hidden state에 누적함으로써, 단어 간 순서와 문맥을 모델이 직접 학습할 수 있도록 설계되었다는 점에서 중요한 의미를 가진다.

이로 인해 RNN은 번역, 언어 모델링, 음성 인식 등 다양한 시퀀스 기반 task에서 널리 활용되었고, 자연어 처리 모델의 적용 범위를 크게 확장시켰다. 특히 문장을 시간 흐름에 따라 처리한다는 개념은 이후 등장하는 모든 NLP 모델의 출발점이 되었다.

그러나 실제 학습 과정에서는 장기 의존성 문제, gradient vanishing/exploding, 그리고 순차 처리로 인한 계산 비효율성이라는 한계가 분명히 드러났다. 이러한 문제들은 데이터 규모와 문맥 길이가 커질수록 더욱 심각해졌고, RNN 계열 모델만으로는 현대 NLP의 요구를 충족하기 어렵다는 인식으로 이어졌다.

이러한 한계에 대한 문제의식은 이후 Attention 메커니즘의 등장으로 이어지며, 자연어 처리 모델 구조에 중요한 전환점을 제공하게 된다.

---
