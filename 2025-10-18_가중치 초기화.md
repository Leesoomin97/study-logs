# 🧩 가중치(weight) 초기화

딥러닝 모델은 학습을 시작할 때 모든 가중치를 랜덤하게 설정한 후 시작한다.  
그런데 이 처음 세팅이 잘못될 경우, 아무리 좋은 옵티마이저를 써도 학습이 망가진다.

너무 큰 값이면 **loss가 폭주**하며,  
너무 작은 값이면 **gradient가 사라진다.**  
결과적으로 잘못된 가중치 설정은 모델이 아예 학습을 못 하게 만드는 것이다.

그래서 **가중치 초기화란 학습이 제대로 굴러가기 위한 출발선 정렬**이라고 할 수 있다.

---

## 2. 주요 초기화 방식

| 초기화 방식 | 핵심 개념 | 적합한 활성화 함수 | 특징 |
|--------------|------------|------------------|------|
| **Zero Init (0으로 초기화)** | 모든 가중치를 0으로 시작 | 없음 | 모든 뉴런이 똑같이 움직여 학습 불가능 |
| **Random Init (무작위)** | 단순히 랜덤 분포로 시작 | 보통 ReLU/Tanh | 너무 크거나 작으면 불안정 |
| **Xavier (Glorot) 초기화** | 입력, 출력 노드 수의 균형 고려 | Sigmoid, Tanh | 초창기 기본값, 안정적인 분포 |
| **He 초기화 (Kaiming)** | 입력 노드 수 기준 분산 조절 | ReLU, LeakyReLU | 현재 가장 널리 쓰이는 표준 |
| **LeCun 초기화** | 출력 노드 수 중심 | SELU | Self-Normalizing 구조용 |
| **Orthogonal 초기화** | 직교 행렬로 구성 | RNN | 장기 의존성 유지에 유리 |

---

## 3. 주요 초기화 방식 요약

- **Xavier**: 입력과 출력의 밸런스를 잘 맞추어 시작  
- **He**: ReLU 사용 시 초반 신호 손실을 줄이기 위하여 강하게 시작  
- **LeCun**: 자기 정규화(Self-Normalizing)을 고려하여 안정적으로 시작  
- **Orthogonal**: 서로의 간섭을 피하기 위하여 독립적으로 시작  

---

## 4. 옵티마이저와의 관계

가중치 초기화는 **출발점을**, 옵티마이저는 **출발 이후의 움직임을** 결정한다.  

- 초기화가 잘못되면 옵티마이저가 방향을 잃고 헤맨다.  
- 초기화가 적절하면 옵티마이저가 부드럽게 수렴한다.  

그래서 실무에서는 대부분 **He Initialization + AdamW** 조합이 기본 세팅이다.

---

## 5. 분야별 가중치 초기화 사용

### (1) 컴퓨터 비전 (CNN)
- 대표 모델: **ResNet, EfficientNet**  
- 대부분 **ReLU + He Initialization** 사용  
- 이유: 이미지 데이터는 픽셀 값의 분포가 넓기 때문에  
  신호가 중간층에서 사라지지 않도록 **입력 기준 분산 보정(He)** 이 필수.  



### (2) 자연어 처리 (NLP)
- 대표 모델: **Transformer, BERT, GPT**  
- **Embedding Layer**는 보통 `Uniform(-0.1, 0.1)`  
- **Linear / Attention Layer**는 **Xavier** 또는 **Orthogonal**  
- 이유: 단어 벡터 간 분산이 중요하고,  
  Attention은 행렬 곱 연산이 많아서 **직교 안정성(Orthogonal)** 이 필요하다.  



### (3) 시계열 / RNN 계열
- 대표 모델: **LSTM, GRU**  
- 가중치 폭주가 치명적이라 **Orthogonal 초기화**가 기본.  
- 입력 게이트 쪽은 **Xavier**, 순환 게이트는 **Orthogonal** 조합도 많음.  
- 이 덕분에 장기 시정(Long-term dependency)을 안정적으로 학습 가능.  



### (4) 추천 시스템 / Tabular 데이터
- 대표 모델: **DeepFM, DCN, Wide&Deep**  
- 주로 **He Initialization (Dense Layer)** + **Uniform (Embedding Layer)**  
- 데이터는 희소하지만 피처 수가 많기 때문에  
  적당한 분산을 가진 초기화가 학습 효율을 높인다.  



### (5) 생성 모델 / GAN
- 대표 모델: **DCGAN, StyleGAN**  
- Generator → `Normal(0, 0.02)`  
- Discriminator → **Xavier or He**  
- GAN은 민감해서 초기화가 잘못되면  
  훈련이 터지거나 Generator가 Collapse(붕괴) 현상을 일으킨다.  



## 6. 핵심 요약

- 가중치 초기화는 **딥러닝 학습의 출발선 정렬**이다.  
- 잘못된 초기화는 아무리 좋은 옵티마이저로도 효과 없다.  
- **ReLU 계열은 He**, **Sigmoid/Tanh 계열은 Xavier**  
- **RNN은 Orthogonal**, **Embedding은 Uniform**  
- **Vision → He**, **NLP → Xavier/Orthogonal**, **Time Series → Orthogonal**
