# 1. 머리글

Transformer 기반 모델의 등장은 대규모 사전학습(Pretraining) 성능의 핵심이 되는 패러다임을 만들었다. 하지만 사전학습 이후 모델을 실제 태스크에 적용하는 방식은 하나로 결정되지 않았다.

BERT 계열은 Fine-tuning을 중심으로 발전했고, GPT 계열은 In-Context Learning(ICL)이라는 전혀 다른 사용 방식을 만들어냈다.

이번 글에서는 두 접근이 같은 출발점에서 어떻게 다른 선택을 했는지, 그리고 이 차이가 모델 활용 방식과 시스템 설계에 어떤 영향을 주었는지를 구조적으로 비교하고자 한다.

---

# 2. Pretraining의 공통 목적

Pretraining의 목적은 대규모 언어 데이터를 통해 모델이 통계적 구조와 일반적 패턴을 학습하도록 만드는 것이다. 이를 수식적으로 보면, 언어 모델은 다음 확률을 근사한다.

(여기에 이미지 또는 수식 자리)

이 과정에서 모델은 문법적 제약, 단어 간 결합 패턴, 문맥에 따른 의미 변화를 파라미터 내부에 압축한다. 문제는 이렇게 학습된 표현을 태스크에 어떻게 연결할 것인가이다.

---

# 3. Fine-tuning(파라미터를 태스크에 맞게 조정)

Fine-tuning은 사전학습된 모델의 파라미터를 특정 태스크 데이터로 추가 학습하는 방식이다. BERT 계열 모델에서 가장 전형적으로 사용되는 방식이기도 하다.

Fine-tuning의 구조는 다음과 같다.

1. Pretrained Encoder로 입력을 임베딩  
2. 태스크 전용 Head(classification, span prediction 등) 추가  
3. 전체 또는 일부 파라미터를 업데이트하며 학습  

이 방식의 핵심은 표현 자체를 태스크에 맞게 재배치한다는 점이다.

---

# 4. Fine-tuning의 장점과 약점

**장점**
- 태스크 특화 성능이 높음  
- 비교적 적은 데이터로도 안정적인 수렴  
- 평가 지표 최적화가 명확함  

**구조적 제약**
- 태스크마다 별도 모델 관리 필요  
- 새로운 태스크마다 재학습 필요  
- 데이터 분포 변화에 취약  

결과적으로, Fine-tuning은 성능은 높지만 운영 비용이 큰 방식이라고 볼 수 있다.

---

# 5. In-Context Learning(파라미터를 고정 후 문제 해결 방식)

In-Context Learning(ICL)은 GPT 계열 모델에서 관찰된 현상이다. 모델의 파라미터를 업데이트하지 않고, 입력 문맥(prompt)만으로 태스크를 수행한다.

형식은 단순하다. 입력에 **태스크 설명 + 예시**를 함께 제공하면, 모델은 이를 조건으로 다음 토큰을 생성한다.  

수식적으로 볼 때, ICL은 여전히 동일한 언어 모델 확률을 따른다고 할 수 있다.

(여기에 이미지 또는 수식 자리)

중요한 점은, **태스크 정보가 파라미터가 아니라 입력 시퀀스에 포함된다**는 것이다.

---

# 6. 왜 ICL이 가능해졌는가

ICL은 새로운 학습 방식이라기보다, 대규모 사전학습의 결과로 드러난 능력에 가깝다. 이는 다음 조건이 동시에 충족되며 가능해졌다.

- 매우 큰 모델 용량  
- 다양한 태스크를 포함한 대규모 데이터  
- Autoregressive 학습 구조  

모델은 학습 과정에서 입력-출력 패턴을 문맥에서 추론하는 방법 자체를 학습하게 되었고, 그 결과 프롬프트 내 예시를 암묵적인 학습 신호로 해석할 수 있게 되었다.

---

# 7. Fine-tuning과 ICL의 구조적 차이

| 구분 | Fine-tuning | In-Context Learning |
|------|-------------|---------------------|
| 파라미터 업데이트 | O | X |
| 태스크 정보 위치 | 모델 파라미터 | 입력 문맥 |
| 배포 방식 | 태스크별 모델 | 단일 모델 |
| 데이터 요구 | 라벨 데이터 필요 | 예시 텍스트 |
| 운영 비용 | 높음 | 낮음 |

이 차이는 단순한 학습 기법의 차이가 아니라, **모델을 시스템에서 사용하는 방식 자체의 차이**이다.

---

# 8. 언제 Fine-tuning과 ICL이 유리한가

## Fine-tuning이 유리한 경우
- 태스크가 고정적이고 명확한 경우  
- 평가 지표 최적화가 중요한 경우  
- 실시간 비용이 중요한 경우  
- (예: 문서 분류, 검색 랭킹, 의학/법률 특화 모델)

## ICL이 유리한 경우
- 태스크가 자주 바뀌는 경우  
- 빠른 실험과 프로토타이핑이 필요한 경우  
- 하나의 모델로 다양한 작업을 처리해야 하는 경우  
- (예: 대규모 언어 모델 기반 서비스)

---

# 9. 마무리

Fine-tuning과 In-Context Learning은 사전학습 이후의 두 가지의 상이한 발전 경로이다. Fine-tuning은 모델을 태스크에 맞게 변형하는 전략이고, ICL은 모델을 그대로 두고 문맥으로 조정하는 전략이다.

이 차이는 이후 **Prompt Engineering**, **Instruction Tuning**, **RLHF** 등 후속 기법의 방향을 결정짓는 출발점이 된다.
