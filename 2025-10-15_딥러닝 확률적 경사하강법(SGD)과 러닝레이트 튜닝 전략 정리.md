# 1. 러닝레이트의 본질

**Learning rate**는 기울기에 따라 얼마나 크게 움직일지를 결정하는 하이퍼파라미터이다.  
너무 크면 손실이 발산하고, 너무 작으면 수렴은 하지만 너무 느려진다.  
따라서 빠르면서도 안정된 수렴을 위해 **적절한 값을 찾는 것이 핵심**이다.

---

## 2. 데이터 관점에서의 고려 요소

EDA 단계에서 얻을 수 있는 정보는 러닝레이트 설정의 출발점이 된다.

| 요소 | 설명 | 튜닝 방향 |
|------|------|-----------|
| 스케일(Scale) | 피처 값의 범위가 크면 gradient 폭발 위험 | 정규화(Standard/MiniMax) 적용 후 Ir ↑ |
| 노이즈(Noise) | 데이터에 노이즈가 많을수록 gradient 불안정 | Ir ↓, 모멘텀↑ 또는 Adam 계열 사용 |
| 표본 수 | 샘플이 많으면 배치를 키워도 안정적 | Ir은 배치 크기로 비례하여 증가 가능 |

---

## 3. 모델 관점에서의 고려 요소

러닝레이트는 손실 함수의 지형(curvature)과 모델 구조에 따라 달라진다.

| 상황 | 전략 |
|------|------|
| Convex한 손실 함수(Linear, Logistic 등) | 일정한 Ir로도 수렴 가능 |
| 비선형 딥러닝(Deep Net) | Adaptive optimizer(Adam 등) 필수 |
| 초기 가중치가 큼 | Ir ↓ (gradient 폭발 방지) |
| 강한 정규화 적용 | Ir ↑ (gradient 크기가 줄어듦) |

---

## 4. 러닝레이트 튜닝 주요 전략

### (1) Learning Rate Finder
- `1e-7 → 1`까지 지수적으로 증가시키며 손실 변화를 관찰.  
- 손실이 급격히 떨어지다가 다시 폭발하기 직전의 지점이 sweet spot.  
- **손실 급감 시작 * 0.5 ≒ 적정 초기 Ir**

---

### (2) Warm-up + Decay
초반엔 작은 Ir로 안정적 시작 → 이후 점진적으로 감소.

- **Step Decay:** 일정 epoch마다 `Ir *= 0.1`  
- **Exponential Decay:** `Ir = Ir0 × exp(-k · epoch)`  
- **Cosine Annealing:** 코사인 곡선처럼 서서히 감소  

> 대형 모델, noisy 데이터에 안정적

---

### (3) Cyclical / One-Cycle Policy
러닝레이트를 주기적으로 변동시켜 **local minima 탈출 유도**

- **Cyclical LR:** `Ir_min → Ir_max → Ir_min` 반복  
- **One-Cycle Policy:** 초반 급격히 상승 → 후반 완만히 감소  

> 빠른 수렴 + regularization 효과

---

### (4) Adaptive Optimizers

| 옵티마이저 | 특징 | 권장 설정 |
|-------------|------|-----------|
| Adam | 안정적, 튜닝 쉬움 | Ir=1e-3, β1=0.9, β2=0.999 |
| AdamW | L2 규제 보강, 일반화 향상 | Ir=1e-3, weight_decay=1e-4~1e-2 |
| RMSProp | 비정형 데이터에서 효과적 | Ir=1e-3, decay=0.9 |
| SGD + Momentum | 일반화 우수, 튜닝 난이도 ↑ | Ir=0.05, momentum=0.9 |

---

## 5. 딥러닝에서의 학습률 설계

딥러닝은 **곡선 형태의 스케줄링이 매우 중요**하다.

| 전략 | 주요 아이디어 | 적용 예시 |
|------|----------------|-----------|
| Warm-up | 초기 gradient 폭발 방지 | BERT, GPT |
| Cosine Decay | 후반부 미세 수렴 | Vision Transformer |
| One-Cycle | 빠른 수렴 + 일반화 | CNN, DCN, Tabular NN |

> 예: **Warm-up 5% + Cosine Decay** 조합이 Transformer의 표준 설정

---

## 6. 전통적 모델과 딥러닝의 차이

| 구분 | 전통적 모델 | 딥러닝 |
|------|---------------|--------|
| 손실 형태 | Convex | Non-convex |
| Ir 조정 방식 | 일정 or 단순 감쇠 | 스케줄러 기반 (warm-up, cycle 등) |
| 일반적 범위 | 0.01 ~ 0.1 | 1e-4 ~ 1e-2 |
| 옵티마이저 | SGD, Adagrad | Adam, AdamW, RMSProp |

---

## 7. 실전 튜닝 절차

1. **LR Finder로 탐색** (`1e-7 ~ 1e-1` sweep)  
2. **Base Train:** 손실 진동 / 수렴 속도 관찰  
3. **Scheduler 선택:** warm-up / cosine / step-decay  
4. **Batch 크기 변경 시 보정:**  
   `Ir_new = Ir_base * √(batch_new / batch_base)`  
5. **Fine-tune:** weight decay, dropout 조정

---

## 8. 상황별 러닝레이트 전략 요약

| 상황 | 추천 전략 |
|------|-------------|
| 데이터 노이즈 많음 | 작은 Ir + 모멘텀 or Adam |
| 손실 진동 심함 | warm-up + decay |
| 수렴 느림 | LR Finder로 상한 탐색 |
| 과적합 발생 | Cyclical LR / Dropout ↑ |
| 큰 배치 사용 | Ir ∝ √(batch size) 증가 |
| Transformer 류 | Warm-up + Cosine Decay |

---

## 9. 기억해야 할 점

러닝레이트는 **숫자가 아니라 곡선의 형태로 설계하는 하이퍼파라미터**이다.  
데이터는 초깃값의 단서, 모델은 곡선의 모양을 결정한다.  
최적의 러닝레이트는 **실험 속에서, 손실 곡선이 가장 아름답게 흐를 때** 드러난다.

---

#딥러닝 #하이퍼파라미터 #러닝레이트 #Deeplearning #Learnigrate #튜닝전략 #Tuning #튜닝
