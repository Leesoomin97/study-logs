# 🩵 데이터 증강기법 완전정복 (Image & Text Augmentation)

## 1. 머리글

딥러닝 모델의 성능은 데이터의 양과 다양성에 크게 의존한다. 하지만 현실에서는 데이터 수집의 제한이 있어 항상 데이터가 부족하다. 이때 필요한 것이 바로 데이터 증강(Data Augmentation)이다.

데이터 증강이란 기존 데이터를 인위적으로 변형해 새로운 학습 샘플을 만들어내는 기술이다.

이번 글에서는 이미지와 텍스트 데이터를 중심으로 어떻게 데이터를 늘리고, 모델의 일반화 성능을 높이는지를 구체적으로 살펴본다.

---

## 2. 데이터 증강이란?

데이터 증강이란 기존 데이터를 변형하여 새로운 학습 샘플을 생성하는 기법이다. 주로 데이터가 부족하거나, 모델이 특정 패턴에 과적합되는 것을 방지하기 위해 사용된다.

데이터 증강의 목적은 단순히 데이터를 많이 만드는 것이 아니라 모델이 다양한 변형 상황에서도 본질적 패턴을 인식하도록 학습시키는 것이다.

---

## 3. 이미지 데이터 증강(Image Augmentation)

### (1) 목적
- 모델이 다양한 시각적 변화에 잘 대응하도록 돕는다.  
- 과적합(overfitting)을 방지하고 일반화 능력을 향상시킨다.

---

### (2) 주요 기법

| 범주 | 기법 | 설명 |
|------|------|------|
| 기하학적 변환 | 회전(Rotation), 뒤집기(Flip), 이동(Translation), 확대/축소(Zoom), 자르기(Crop), 기울이기(Shear) | 위치, 각도, 크기 변화를 통해 시각적 다양성 확보 |
| 색상/밝기 변환 | 밝기(Brightness), 대비(Contrast), 채도(Saturation), 색조(Hue) | 조명, 환경 변화에 강인한 모델 학습 |
| 노이즈 주입 | Gaussian Noise, Salt&Pepper Noise | 입력 왜곡에 견디는 강건성(Robustness) 향상 |
| Cutout/Random Erasing | 임의 영역을 가려 일부 정보 제거 | 일부 객체 손실에도 예측 가능하도록 학습 |
| Mixup/CutMix | 서로 다른 이미지를 섞거나 일부 패치 교환 | 클래스 경계를 부드럽게 하여 일반화 향상 |
| AutoAugment/RandAugment | 증강 조합과 강도를 자동 탐색 | 최적 증강 정책을 자동으로 찾아주는 알고리즘 |

---

### (3) 예시 코드

```python
from torchvision import transforms

train_transform = transforms.Compose([
    transforms.RandomResizedCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
    transforms.RandomRotation(15),
    transforms.ToTensor(),
])
```

---

### (4) 유의사항

① **과도한 증강은 독이 될 수 있다.**  
이미지의 의미가 변하거나 객체가 손상되면 오히려 학습이 불안정해진다.  
예: 얼굴 인식 모델에 180도 회전 → 사람 아닌 것으로 인식할 수 있음.

② **도메인에 맞는 증강만 사용하기**  
의료 영상, 위성사진 등은 일반 이미지보다 구조적 제약이 많다.  
예: X-ray는 색상 변형보다는 좌우 뒤집기만 허용

③ **Validation/Test에는 절대 증강을 적용하지 않는다.**  
평가 데이터는 원본 그대로 사용해야 공정한 성능 비교가 가능하다.

---

## 4. 텍스트 데이터 증강(Text Augumentation)

### (1) 목적

문장 수가 적거나 클래스 불균형인 경우 언어적 다양성을 확보하기 위해 사용한다. 또한 모델이 의미적 유연성(Semantic Robustness)을 배우도록 유도한다.

---

### (2) 주요 기법

| 범주 | 기법 | 설명 |
|------|------|------|
| 단어 수준(Word-level) | 동의어 치환(Synonym Replacement), 무작위 삽입(Random Insertion), 무작위 삭제(Random Deletion), 단어 순서 변경(Random Swap) | 문장 구조를 유지하면서 변형 |
| 문장 수준(Sentence-level) | 역번역(Back Translation), 패러프레이징(Paraphrasing) | 번역 모델이나 LLM으로 의미는 유지하고 표현만 바꾸기 |
| 컨텍스트 기반 | BERT, RoBERTa 등으로 문맥을 고려한 단어 치환 | 주변 문맥에 맞는 자연스러운 증강 |
| 노이즈 주입 | 맞춤법 오류, 오타 추가 | 실제 사용자 입력(트윗, 댓글 등)에 대응하는 강건성 향상 |

---

### (3) 예시 코드

```python
import nlpaug.augmenter.word as naw

# BERT 기반 문맥 치환
aug = naw.ContextualWordEmbsAug(model_path='bert-base-uncased', action="substitute")
aug.augment("The movie was incredibly good and emotional.")
```

---

### (4) 유의사항

① **문맥 왜곡 주의**  
증강 후 문장의 감정, 부정/긍정 방향이 바뀌면 데이터 품질이 무너진다.  
예: I hate this → I like this (❌)

② **비정형 텍스트는 노이즈보다 의미 보존 우선**  
트윗, 댓글 등에서는 문맥 유지가 더 중요하다.

③ **라벨 유지 확인 필수**  
증강 후에도 기존 라벨(Label)이 여전히 유효한지 검증해야 한다.

---

## 5. 실제 활용 사례

| 분야 | 적용 예시 | 효과 |
|------|------------|------|
| 이미지 분류(Image Classification) | CIFAR-10, ImageNet에서 AutoAugment 사용 | Top-1 정확도 1~2% 향상 |
| 의료 영상(Medical Imaging) | X-ray 이미지의 랜덤 회전, 좌우 반전 | 소량 데이터에서도 진단 정확도 향상 |
| 자율 주행(Autonomous Driving) | 날씨, 조명 증강(Rain, Fog, Brightness) | 실제 도로 환경에서 강건성 개선 |
| 텍스트 감정 분석(Sentiment Analysis) | Back Translation으로 문장 수 확장 | 라벨 불균형 완화, F1 Score 향상 |
| 챗봇/QA 데이터셋 구축 | GPT 기반 패러프레이징으로 질문 다양화 | 사용자 발화 대응력 증가 |

---

## 6. 정리 및 비교

| 구분 | 주요 목적 | 대표 기법 | 특징 |
|------|------------|------------|------|
| 이미지 증강 | 다양한 시각적 패턴 학습 | Flip, Crop, Rotation, Mixup, AutoAugment | 시각적 다양성 확보, 과적합 방지 |
| 텍스트 증강 | 언어 표현 다양성 확보 | Synonym Replacement, Back Translation, Paraphrasing | 데이터 불균형 해소, 문맥 일반화 향상 |

---

## 7. 마무리

데이터 증강은 적은 데이터로도 이전보다 더 똑똑한 모델을 만드는 기술이다. 이는 단순한 이미지 변형이나 단어 치환을 넘어, 데이터의 본질적 다양성을 확장하는 과정이라고 볼 수 있다.

하지만 목적 없이 무작정 증강을 적용하면 오히려 모델이 혼란스러워질 수 있다. 따라서 도메인에 맞는 전략적 증강 조합이 필요하다. 이에 다음 스터디 로그에서는 새로운 데이터 생성 없이 모델 성능을 높이는 방법인 전이학습과 자기지도학습을 공부해 볼 예정이다.
