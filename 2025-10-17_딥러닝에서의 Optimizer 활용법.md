# Optimizer란?

옵티마이저란 딥러닝 모델이 학습할 때, 손실(loss)을 줄이기 위해 가중치(weight)를 조금씩 바꿔나가야 하는데, 이때 '얼마나, 어떤 방향으로 바꿀 것인가'를 결정해 주는 알고리즘을 말한다.

쉽게 말하면, 손실이 최소가 되는 지점을 찾아가는 길잡이다. 언덕을 내려가는 등산가처럼, 모델이 더 나은 방향을 향하도록 돕는 역할을 한다.

---

## 대표적인 옵티마이저들의 성격

| 옵티마이저 | 성격 비유 | 설명 |
|-------------|------------|------|
| **SGD** | 전통적인 등산가 | 기울기 방향으로 한 걸음씩 이동. 단순하지만 느리고, 지역 최소값에 갇히기 쉬움. |
| **Momentum** | 관성 있는 스케이트 보더 | 이전 이동 방향을 기억해 가속도를 붙인다. 울퉁불퉁한 지형에서도 빨리 내려감. |
| **NAG** | 미래를 살짝 내다보는 스케이터 | 이동 전에 미리 앞을 보고 방향을 미세 조정. 더 빠르고 안정적. |
| **Adagrad** | 공부 잘하는 학생 | 파라미터마다 학습률을 다르게 설정. 희귀한 피처에 빠르게 적응하지만 후반엔 너무 느려짐. |
| **RMSProp** | 기억력 좋은 학생 | 최근 변화만 적당히 반영. 시계열(RNN)에서 특히 유용. |
| **Adam** | 종합형 천재 | Momentum + RMSProp의 장점을 섞은 하이브리드. 대부분의 모델 기본값. |
| **AdamW** | 다이어트 전문가 | Adam에 정규화(Weight Decay)를 추가해 과적합을 방지. 현재 가장 널리 쓰이는 표준. |

---

## 옵티마이저 간 비교 요약

- **SGD**: 느리지만 가장 기본적이고 일반화에 강함  
- **Momentum / NAG**: 안정적인 수렴, 조금 더 빠름  
- **RMSProp**: 시계열이나 변화가 많은 데이터에 적합  
- **Adam**: 빠르고 효율적, 대부분의 모델 기본값  
- **AdamW**: 정규화 효과로 과적합 방지 → 현재 표준 세팅  

---

## CTR 문제에서는?

CTR처럼 양성 비율이 매우 낮은 불균형 데이터에서는 보통 **AdamW**나 **RMSProp**를 많이 사용한다.

이유는 다음과 같다.

- 학습률을 자동으로 조절해서 희귀한 클릭 신호를 놓치지 않는다.  
- AdamW는 Weight Decay로 과적합을 막는다.  

실제 Toss, Criteo, Avazu 등 CTR 경진대회에서도 AdamW는 거의 기본 세팅으로 사용된다.

---

## 상황별 추천 요약

| 상황 | 추천 옵티마이저 |
|------|----------------|
| 이미지 / Vision 모델 | SGD + Momentum |
| Tabular / CTR / NLP 모델 | AdamW |
| 시계열 / 순환구조(RNN) | RMSProp |
| 희귀 피처가 많은 NLP / Embedding 초기 학습 | Adagrad |

**결과적으로**

- 데이터가 크고 안정성을 원하면 → **SGD 계열**  
- 빠르고 효율적인 수렴을 원하면 → **Adam 계열**  
- 불균형, 희소 데이터 환경이면 → **AdamW / RMSProp**

---

## 학습률(Learning Rate)가 더 중요하다

사실 옵티마이저보다는 **학습률 조절**이 모델의 성능을 좌우한다.

- 학습률이 너무 크면 → Loss가 튀어서 폭주  
- 너무 작으면 → 학습이 멈춤  
- **Scheduler**를 사용해 점점 줄이는 방식이 안정적  

결과적으로 옵티마이저는 도구일 뿐, 모델이 제대로 학습하게 만드는 주요 키는 **Learning Rate**이다.

---

## 마무리

옵티마이저는 모델이 어떻게 학습해야 할지 알려주는 **길 찾기 알고리즘**으로서, 가장 보편적이고 안정된 선택은 **AdamW**이다.  
또한 **SGD**는 여전히 신뢰할 수 있는 기본선이며, **CTR이나 불균형 데이터에서는 RMSProp/AdamW**의 조합이 효과적으로 이용된다.

다만, 옵티마이저는 어디까지나 보조적인 수단일 뿐, **실질적으로 중요한 것은 학습률**이라는 것을 염두에 두어야 한다.
